training_args:
  # The NULLs are placeholders for the user to fill in by optuna hyperparameter search
  # This will be replaced in next iteration by a function
  num_train_epochs: null
  learning_rate: null
  per_device_train_batch_size: null
  per_device_eval_batch_size: null
  disable_tqdm: false

optuna:
  n_trials: 10 # increase this to 10 or so
  n_seeds: 5

optuna_search_space:
  learning_rate:
    low: 1e-5
    high: 1e-3
    type: loguniform
  batch_size:
    values: [16, 32, 64, 128]
  num_train_epochs:
    low: 5
    high: 15
    type: int

adapter_fusion:
  adapter1_path: adapters/training_output/roberta-base_citation_intent_seq_bn_training_adapter_v01_best/trial_1/seed_128
  adapter2_path: adapters/training_output/roberta-base_chemprot_seq_bn_training_adapter_v01_best/trial_1/seed_42

output_dir: ./training_output

defaults:
  - base_config.yaml
