{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory to be project path\n",
    "import os\n",
    "os.chdir('/Users/datagero/Documents/local_repos/gatech/Deep Learning/Project/cs-7643-efficiencylane')\n",
    "\n",
    "# Add to python path\n",
    "import sys\n",
    "sys.path.append('/Users/datagero/Documents/local_repos/gatech/Deep Learning/Project/cs-7643-efficiencylane/cs_7643_efficiencylane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and Loader\n",
    "from transformers import RobertaConfig, TextClassificationPipeline, RobertaForSequenceClassification\n",
    "from data_loaders.citation_intent_data_loader import CSTasksDataLoader\n",
    "from adapters import AutoAdapterModel, RobertaAdapterModel\n",
    "import torch\n",
    "\n",
    "model_variant = \"roberta-base\"\n",
    "\n",
    "dataset_name = \"sciie\"\n",
    "adapter_path = \"adapters/training_output/roberta-base_sciie_double_seq_bn_training_adapter_v01_best/trial_2/seed_9091\"\n",
    "\n",
    "print(\"Loading Dataset...\")\n",
    "loader = CSTasksDataLoader(model_name=\"roberta-base\",\n",
    "                                dataset_name=dataset_name,\n",
    "                                path=f\"data/{dataset_name}/\",\n",
    "                                checkpoint_path=f\"data/{dataset_name}/processed_dataset.pt\")\n",
    "\n",
    "dataset = loader.load_dataset(overwrite=False)\n",
    "num_labels = loader.num_labels\n",
    "print(\"num_labels:\", num_labels)\n",
    "\n",
    "device = \"cpu\"#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# ======================================================\n",
    "# Model & Adapter Config\n",
    "# ======================================================\n",
    "# Set up training for the Model and Adapter\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "\n",
    "print(\"Initialising Model...\")\n",
    "model = RobertaAdapterModel.from_pretrained(model_variant, config=config)\n",
    "model.to(device)\n",
    "\n",
    "print(\"Adding Adapter...\")\n",
    "adapter_name = model.load_adapter(adapter_path)\n",
    "model.set_active_adapters(adapter_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict labels for a list of texts\n",
    "def classify_texts(model, tokenizer, texts):\n",
    "    # Prepare the model input\n",
    "    encoded_inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = encoded_inputs['input_ids'].to(model.device)\n",
    "    attention_mask = encoded_inputs['attention_mask'].to(model.device)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        # print(outputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Convert predictions to labels (if needed, map these indices back to label names)\n",
    "    return predictions.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get all unique labels in the dataset\n",
    "unique_labels = torch.unique(torch.tensor(dataset['test']['labels']))\n",
    "text_by_label = {}\n",
    "\n",
    "# Extract corresponding texts\n",
    "for label in unique_labels:\n",
    "    inx_label = [idx for idx, val in enumerate(dataset['test']['labels']) if val == label.item()]\n",
    "    text_by_label[label.item()] = [dataset['test']['text'][i] for i in inx_label]\n",
    "\n",
    "print(text_by_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_by_label = {}\n",
    "\n",
    "# Load the tokenizer from the data loader\n",
    "tokenizer = loader.tokenizer\n",
    "\n",
    "# Classify texts and store predictions\n",
    "for label, texts in text_by_label.items():\n",
    "    if texts:\n",
    "        predictions = classify_texts(model, tokenizer, texts)\n",
    "        predictions_by_label[label] = predictions\n",
    "    else:\n",
    "        predictions_by_label[label] = []\n",
    "\n",
    "# Print predictions for each label\n",
    "for label, predictions in predictions_by_label.items():\n",
    "    print(f\"Label {label} Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Label decoding\n",
    "label_decoder = {v: k for k, v in loader.label_encoder.items()}\n",
    "\n",
    "# Aggregate true labels and their predictions\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for true_label, preds in predictions_by_label.items():\n",
    "    true_labels.extend([true_label] * len(preds))\n",
    "    predicted_labels.extend(preds)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Generate label names using the label decoder\n",
    "axis_labels = [label_decoder[i] for i in sorted(label_decoder.keys())]\n",
    "\n",
    "# Create a figure to plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', xticklabels=axis_labels, yticklabels=axis_labels, linewidths=.5, linecolor='white')\n",
    "\n",
    "# Beautifying the plot\n",
    "plt.title('Confusion Matrix', fontsize=20, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Labels', fontsize=14, labelpad=10)\n",
    "plt.ylabel('True Labels', fontsize=14, labelpad=10)\n",
    "plt.xticks(fontsize=12, rotation=45, ha='right')\n",
    "plt.yticks(fontsize=12, rotation=0, va='center')\n",
    "\n",
    "# Draw the grid lines\n",
    "ax.set_xticks(np.arange(cm.shape[1]+1)-.5, minor=True)\n",
    "ax.set_yticks(np.arange(cm.shape[0]+1)-.5, minor=True)\n",
    "ax.grid(which=\"minor\", color=\"white\", linestyle='-', linewidth=0.3)\n",
    "ax.tick_params(which=\"minor\", size=0)\n",
    "\n",
    "# Avoid the grid lines cutting through the boxes\n",
    "ax.set_xticklabels(axis_labels, rotation=45, horizontalalignment='right')\n",
    "ax.set_yticklabels(axis_labels, rotation=0, verticalalignment='center')\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(\"project_report/resources/confusion_matrix_predictions_roberta-base_sciie_double_seq_bn.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import TrainingArguments, EvalPrediction\n",
    "from adapters import AdapterTrainer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=0.0009529536457150203,\n",
    "    num_train_epochs=9,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=200,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "def macro_f1(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"macro_f1\": f1_score(p.label_ids, preds, average='macro')}\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"dev\"],\n",
    "    compute_metrics=macro_f1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(dataset[\"test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs-7643-efficiencylane-mvGhKmLY-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
