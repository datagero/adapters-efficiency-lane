{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory to be project path\n",
    "import os\n",
    "os.chdir('/home/hice1/avizcaino3/scratch/repos/CS-7643-EfficiencyLane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/avizcaino3/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/hice1/avizcaino3/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/hice1/avizcaino3/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/hice1/avizcaino3/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/hice1/avizcaino3/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Dataset loaded from checkpoint.\n",
      "num_labels: 6\n",
      "Initialising Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Adapter...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and Loader\n",
    "from transformers import RobertaConfig, TextClassificationPipeline, RobertaForSequenceClassification\n",
    "from data_loaders.citation_intent_data_loader import CSTasksDataLoader\n",
    "from adapters import AutoAdapterModel, RobertaAdapterModel\n",
    "import torch\n",
    "\n",
    "model_variant = \"roberta-base\"\n",
    "\n",
    "# dataset_name = \"sciie\"\n",
    "# adapter_path = \"adapters/training_output/roberta-base_sciie_double_seq_bn_training_adapter_v01_best/trial_2/seed_9091\"\n",
    "\n",
    "dataset_name = \"citation_intent\"\n",
    "adapter_path = \"adapters/training_output/roberta-base_citation_intent_seq_bn_training_adapter_v01_best/trial_2/seed_9091\"\n",
    "\n",
    "print(\"Loading Dataset...\")\n",
    "loader = CSTasksDataLoader(model_name=\"roberta-base\",\n",
    "                                dataset_name=dataset_name,\n",
    "                                path=f\"data/{dataset_name}/\",\n",
    "                                checkpoint_path=f\"data/{dataset_name}/processed_dataset.pt\")\n",
    "\n",
    "dataset = loader.load_dataset(overwrite=False)\n",
    "num_labels = loader.num_labels\n",
    "print(\"num_labels:\", num_labels)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# ======================================================\n",
    "# Model & Adapter Config\n",
    "# ======================================================\n",
    "# Set up training for the Model and Adapter\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "\n",
    "print(\"Initialising Model...\")\n",
    "model = RobertaAdapterModel.from_pretrained(model_variant, config=config)\n",
    "model.to(device)\n",
    "\n",
    "print(\"Adding Adapter...\")\n",
    "adapter_name = model.load_adapter(adapter_path)\n",
    "model.set_active_adapters(adapter_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Background': 0,\n",
       " 'Extends': 1,\n",
       " 'Motivation': 2,\n",
       " 'CompareOrContrast': 3,\n",
       " 'Uses': 4,\n",
       " 'Future': 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict labels for a list of texts\n",
    "def classify_texts(model, tokenizer, texts):\n",
    "    # Prepare the model input\n",
    "    encoded_inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = encoded_inputs['input_ids'].to(model.device)\n",
    "    attention_mask = encoded_inputs['attention_mask'].to(model.device)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        # print(outputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Convert predictions to labels (if needed, map these indices back to label names)\n",
    "    return predictions.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and NE classification ( Collins and Singer , 1999 ) .', 'The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , Hearst ( 1992 ) ) .', \"For instance , Sells ( 1985 , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''\", \"And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .\", \"Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to Keller and Lapata ( 2003 ) 's conditional probability scores for pseudodisambiguation of ( v , n , n â\\x80² ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) .\"], 1: ['Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', 'Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit', 'Henceforth the collaborative traits of blogs and wikis ( McNeill , 2005 ) emphasize annotation , comment , and strong editing .', 'The ICA system ( Hepple , 2000 ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .', 'To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; Sasajima et al. , 1999 ) .', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .', 'Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .', 'A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( Pereira and Riley , 1997 ; Knight and Graehl , 1998 ) .', 'As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; Chen , 1996 ) .', 'Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Gurevych ( 2005 ) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .', 'Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( Evermann et al. , 2004 ) .', \"Later , Hobbs ( 1979 , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .\", 'Another technique is automatic discovery of translations from parallel or non-parallel corpora ( Fung and Mckeown , 1997 ) .', \"Opposition ( called `` adversative '' or `` contrary-to-expectation '' by Halliday and Hasan 1976 ; cfXXX also Quirk et al. 1972 , p. 672 ) .\", 'A number of applications have relied on distributional analysis ( Harris , 1971 ) in order to build classes of semantically related terms .', 'The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .', 'There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research Group , 2001 ) .', \"For the task of unsupervised dependency parsing , Smith and Eisner ( 2006 ) add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .\", 'There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .', 'For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( Strzalkowski 1994 ) .', 'Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', \"The paradigm is `` write many , read many '' ( Cunningham and Leuf , 2001 ) .\", '2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( Kamp 1975 ; also Section 8.1 of the present article ) .', 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .', \"The names given to the components vary ; they have been called `` strategic '' and `` tactical '' components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , `` planning '' and `` realization '' ( e.g. , McDonald 1983 ; Hovy 1988a ) , or simply `` what to say '' versus `` how to say it '' ( e.g. , Danlos 1987 ; Reithinger 1990 ) .\", 'Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'After the extraction , pruning techniques ( Snover et al. , 2009 ) can be applied to increase the precision of the extracted paraphrases .', 'In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , Bacchiani et al. , 2004 ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .', 'GATE goes beyond earlier systems by using a component-based infrastructure ( Cunningham , 2000 ) which the GUI is built on top of .', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ; Perlis ( 1985 ) describes how first order logic can be augmented with such an operator .', 'description-level lexical rules ( DLRs ; Meurers 1995 ) .5 2.2.1 Meta-Level Lexical Rules .', 'All EBMT systems , from the initial proposal by Nagao ( 1984 ) to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', 'The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and Haberlandt et al. ( 1980 ) .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - .', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .', 'Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( McCray et al. , 1988 ) .', 'Semantic Role labeling ( SRL ) was first defined in Gildea and Jurafsky ( 2002 ) .', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( Garrett , 2005 ) .', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and Kameyama ( 1997 ) .', \"While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( Church and Hanks 1990 ) :\", 'Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J Â¨ appinen and Niemist Â¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', 'For shuffling paraphrases , french alternations are partially described in ( Saint-Dizier , 1999 ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', \"A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( Vendler , 1957 ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )\", 'Watanabe ( 1993 ) combines lexical and dependency mappings to form his generalizations .', 'Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) .', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( Goodman 1996 ) .', 'Goodman ( 1996 , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', 'Pustejovsky ( 1995 ) avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .', 'Hohensee and Bender ( 2012 ) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001b ; Lafferty et al. , 2001 ) .', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing ( 1985 ) , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; Freund et al. 1997 ) .', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) .', 'As a generalization , Briscoe ( 2001 ) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; Dagan et al. , 2009 ) .', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .', 'Discriminant analysis has been employed by researchers in automatic text genre detection ( Biber 1993b ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .', \"In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( Barwise and Perry 1983 ; see our Section 2 ) .\", 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ Galley et al. 2004 ; Chiang et al. 2005 ] ) as well as for', 'In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .'], 2: ['One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( Yamada and Knight , 2001 ) .', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; Munson et al. , 2005 ) .', 'A possible future direction would be to compare the query string to retrieved results using a method similar to that of Tsuruoka and Tsujii ( 2003 ) .', 'In a similar vain to Skut and Brants ( 1998 ) and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( Aronson et al. 2004 ) .'], 3: ['Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .', 'Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .', 'The advantage of tuning similarity to the application of interest has been shown previously by Weeds and Weir ( 2005 ) .', 'Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'For instance , Palmer and Hearst ( 1997 ) report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', 'A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; Bod , 2001 ) .', 'For example , our previous work ( Nakov and Ng , 2009 ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .', 'The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( Levin , 1993 ) , possibly arranged in an inheritance hierarchy .', 'Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; Nivre 2009 ) .', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .', 'Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; Shrestha and McKeown 2004 ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000 ;', 'In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( Blunsom et al. 2009 ) 5 .', 'Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of Kiefer et al. ( 1999 ) , who report large speed-ups from the elimination of disjunction processing during unification .', 'Second , in line with the findings of ( Mehdad et al. , 2010 ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .', 'Therefore , inter-subject correlation is lower than the results obtained by Gurevych ( 2006 ) .', 'The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( Tillmann and Ney , 1997 ) .', 'Zollmann and Venugopal ( 2006 ) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; Krieger and Nerbonne 1992 ;', 'The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; Bod , 2001 ) .', \"The most detailed evaluation of link tokens to date was performed by ( Macklovitch & Hannan , 1996 ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .\", 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( Charniak , 2000 ) .', 'â\\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; Roy and Subramaniam 2006 ) .', 'By contrast , Turkish ( Oflazer et al. , 2003 ; Atalay et al. , 2003 ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', \"The question answering system developed by Chu-Carroll et al. ( 2003 ) belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .\"], 4: ['However , the method we are currently using in the ATIS domain ( Seneff et al. 1991 ) represents our most promising approach to this problem .', 'We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) .', 'Our classification framework , directly inspired by Blum and Chawla ( 2001 ) , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .', 'Training was done on the Penn Treebank ( Marcus et al. , 1993 ) Wall Street Journal data , sections 02-21 .', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( Giuliano , 2007 ) to measure the relatedness between words in the dataset .', \"One approach to this more general problem , taken by the ` Nitrogen ' generator ( Langkilde and Knight , 1998a ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .\", 'where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( Berger et al. , 1996 ) .', '13 We also employed sequence-based measures using the ROUGE tool set ( Lin and Hovy 2003 ) , with similar results to those obtained with the word-by-word measures .', 'In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( Goldberg and Elhadad 2010 ) .', 'ASARES is presented in detail in ( Claveau et al. , 2003 ) .', 'The speech and language processing architecture is based on that of the SRI CommandTalk system ( Moore et al. , 1997 ; Stent et a. , 1999 ) .', 'successfully parses , or until a quitting criterion is reached , such as an upper bound on N. Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( Zue et al. 1991 ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'The Praat tool was used ( Boersma and Weenink , 2009 ) .', 'Our work is inspired by the latent left-linking model in Chang et al. ( 2013 ) and the ILP formulation from Chang et al. ( 2011 ) .', 'We use the TRIPS dialogue parser ( Allen et al. , 2007 ) to parse the utterances .', 'In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( Marom and Zukerman 2007a ) .', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( Nakano et al. , 1999b ) , which is an integrated parsing and discourse processing method .', 'We applied our system to the XTAG English grammar ( The XTAG Research Group , 2001 ) 3 , which is a large-scale FB-LTAG grammar for English .', 'The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; Dzikovska et al. , 2006 ) to represent the state of the world .', 'To model d ( FWi â\\x88\\x92 1 , S â\\x86\\x92 T ) , d ( FWi +1 , S â\\x86\\x92 T ) , i.e. whether Li , S â\\x86\\x92 T and Ri , S â\\x86\\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of Setiawan et al. ( 2009 ) .', 'criteria and data used in our experiments are based on the work of Talbot et al. ( 2011 ) .', 'We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( Hovy et al. , 2006 ) .', 'â\\x80¢ Support vector machines for mapping histories to parser actions ( Kudo and Matsumoto , 2002 ) .', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( Koehn et al. , 2007 ) .', 'Our rules for phonological word formation are adopted , for the most part , from G & G , Grosjean and Gee ( 1987 ) , and the account of monosyllabic destressing in Selkirk ( 1984 ) .', 'Following Miller et al. , 1999 , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .'], 5: ['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( Zhang and Clark , 2007 ) and the perceptron POS tagging model from Collins ( 2002 ) .', 'Previously ( Gerber and Chai 2010 ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .', 'The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; Setiawan et al. , 2009 ) .', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( Andrews et al. , 2009 ; Silberer and Lapata , 2012 ) .', 'We have shown elsewhere ( Jensen and Binot 1988 ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217156/3137747428.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  unique_labels = torch.unique(torch.tensor(dataset['test']['labels']))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get all unique labels in the dataset\n",
    "unique_labels = torch.unique(torch.tensor(dataset['test']['labels']))\n",
    "text_by_label = {}\n",
    "\n",
    "# Extract corresponding texts\n",
    "for label in unique_labels:\n",
    "    inx_label = [idx for idx, val in enumerate(dataset['test']['labels']) if val == label.item()]\n",
    "    text_by_label[label.item()] = [dataset['test']['text'][i] for i in inx_label]\n",
    "\n",
    "print(text_by_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, texts \u001b[38;5;129;01min\u001b[39;00m text_by_label\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m texts:\n\u001b[0;32m----> 9\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         predictions_by_label[label] \u001b[38;5;241m=\u001b[39m predictions\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m, in \u001b[0;36mclassify_texts\u001b[0;34m(model, tokenizer, texts)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# print(outputs)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/adapters/models/roberta/adapter_model.py:70\u001b[0m, in \u001b[0;36mRobertaAdapterModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, head, output_adapter_gating_scores, output_adapter_fusion_attentions, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     63\u001b[0m     inputs_embeds\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, inputs_embeds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), inputs_embeds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     66\u001b[0m )\n\u001b[1;32m     68\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m---> 70\u001b[0m outputs, context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_adapter_gating_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_adapter_gating_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_adapter_fusion_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_adapter_fusion_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43madapter_input_parallelized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madapter_input_parallelized\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# required e.g. for prompt tuning in all models\u001b[39;00m\n\u001b[1;32m     86\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m context\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/adapters/context.py:116\u001b[0m, in \u001b[0;36mForwardContext.wrap.<locals>.wrapper_func\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m output_context \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_context\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    113\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    114\u001b[0m     k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_attributes\n\u001b[1;32m    115\u001b[0m }\n\u001b[0;32m--> 116\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# append output attributes\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/adapters/model_mixin.py:1270\u001b[0m, in \u001b[0;36mModelBaseAdaptersMixin.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;129m@ForwardContext\u001b[39m\u001b[38;5;241m.\u001b[39mwrap\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    826\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    828\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    829\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    830\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    834\u001b[0m )\n\u001b[0;32m--> 835\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    848\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    513\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    514\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         output_attentions,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1564\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1566\u001b[0m     ):\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:455\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    452\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    453\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 455\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/transformers/pytorch_utils.py:242\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:468\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    467\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 468\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/adapters/models/roberta/modeling_roberta.py:159\u001b[0m, in \u001b[0;36mRobertaOutputWithAdapters.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    157\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    158\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> 159\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbottleneck_layer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/adapters/methods/bottleneck.py:348\u001b[0m, in \u001b[0;36mBottleneckLayer.bottleneck_layer_forward\u001b[0;34m(self, hidden_states, residual_input, layer_norm)\u001b[0m\n\u001b[1;32m    345\u001b[0m input_hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    347\u001b[0m state \u001b[38;5;241m=\u001b[39m BottleneckState(hidden_states, residual_input, residual_input, layer_norm)\n\u001b[0;32m--> 348\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_setup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m hidden_states, residual_input, _, _, _ \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m    351\u001b[0m last_adapter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapters[adapter_setup\u001b[38;5;241m.\u001b[39mlast()]\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/adapters/methods/adapter_layer_base.py:472\u001b[0m, in \u001b[0;36mComposableAdapterLayerBase.compose\u001b[0;34m(self, adapter_setup, state)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(adapter_setup, AdapterCompositionBlock):\n\u001b[1;32m    471\u001b[0m     composition_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomposition_to_func_map[\u001b[38;5;28mtype\u001b[39m(adapter_setup)]\n\u001b[0;32m--> 472\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mcomposition_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_setup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m adapter_setup \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter_modules:\n\u001b[1;32m    474\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_single(adapter_setup, state, lvl\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/adapters/methods/adapter_layer_base.py:308\u001b[0m, in \u001b[0;36mComposableAdapterLayerBase.compose_stack\u001b[0;34m(self, adapter_setup, state, lvl)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m adapter_stack_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter_modules:\n\u001b[1;32m    307\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_block(adapter_stack_layer, state)\n\u001b[0;32m--> 308\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_stack_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlvl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid adapter setup: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not a valid adapter name or composition block.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    312\u001b[0m             adapter_stack_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    313\u001b[0m         )\n\u001b[1;32m    314\u001b[0m     )\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/adapters/methods/bottleneck.py:230\u001b[0m, in \u001b[0;36mBottleneckLayer.compose_single\u001b[0;34m(self, adapter_setup, state, lvl)\u001b[0m\n\u001b[1;32m    228\u001b[0m adapter_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapters[adapter_setup]\n\u001b[1;32m    229\u001b[0m context \u001b[38;5;241m=\u001b[39m ForwardContext\u001b[38;5;241m.\u001b[39mget_context()\n\u001b[0;32m--> 230\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43madapter_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresidual_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapter_residual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gating\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_adapter_gating_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m hidden_states, up \u001b[38;5;241m=\u001b[39m layer_output[\u001b[38;5;241m0\u001b[39m], layer_output[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_gating_score(adapter_setup, layer_output[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/adapters/methods/modeling.py:172\u001b[0m, in \u001b[0;36mAdapter.forward\u001b[0;34m(self, x, residual_input, output_gating)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, residual_input, output_gating\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 172\u001b[0m     down \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapter_down\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     up \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter_up(down)\n\u001b[1;32m    175\u001b[0m     up \u001b[38;5;241m=\u001b[39m up \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "predictions_by_label = {}\n",
    "\n",
    "# Load the tokenizer from the data loader\n",
    "tokenizer = loader.tokenizer\n",
    "\n",
    "# Classify texts and store predictions\n",
    "for label, texts in text_by_label.items():\n",
    "    if texts:\n",
    "        predictions = classify_texts(model, tokenizer, texts)\n",
    "        predictions_by_label[label] = predictions\n",
    "    else:\n",
    "        predictions_by_label[label] = []\n",
    "\n",
    "# Print predictions for each label\n",
    "for label, predictions in predictions_by_label.items():\n",
    "    print(f\"Label {label} Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Aggregate true labels and their predictions\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for true_label, preds in predictions_by_label.items():\n",
    "    true_labels.extend([true_label] * len(preds))\n",
    "    predicted_labels.extend(preds)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(6), yticklabels=range(6))\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/avizcaino3/scratch/.cache/poetry/virtualenvs/cs-7643-efficiencylane-mvGhKmLY-py3.8/lib/python3.8/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import TrainingArguments, EvalPrediction\n",
    "from adapters import AdapterTrainer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=0.0009529536457150203,\n",
    "    num_train_epochs=9,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=9,\n",
    "    logging_steps=200,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "def macro_f1(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"macro_f1\": f1_score(p.label_ids, preds, average='macro')}\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"dev\"],\n",
    "    compute_metrics=macro_f1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.5806162357330322,\n",
       " 'eval_macro_f1': 0.7430930528239664,\n",
       " 'eval_runtime': 0.5202,\n",
       " 'eval_samples_per_second': 219.157,\n",
       " 'eval_steps_per_second': 24.992}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.5277773141860962,\n",
       " 'eval_macro_f1': 0.6569663141091713,\n",
       " 'eval_runtime': 0.3963,\n",
       " 'eval_samples_per_second': 350.708,\n",
       " 'eval_steps_per_second': 40.369}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(dataset[\"test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs-7643-efficiencylane-mvGhKmLY-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
