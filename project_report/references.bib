@inproceedings{gururangan2020dont,
  author = {Gururangan, S. et al.},
  title = {Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks},
  booktitle = {ACL},
  year = {2020}
}

@inproceedings{houlsby2019parameter,
  author = {Houlsby, N. et al.},
  title = {Parameter-Efficient Transfer Learning for NLP},
  booktitle = {ICML},
  year = {2019}
}

@article{mccloskey1989catastrophic,
  author = {Mccloskey, M. and Cohen, N. J.},
  title = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
  journal = {Psychology of Learning and Motivation},
  year = {1989}
}

@article{pfeiffer2020adapterhub,
  author = {Pfeiffer, J. et al.},
  title = {AdapterHub: A Framework for Adapting Transformers},
  year = {2020},
  journal = {ArXiv}
}

@article{pfeiffer2020adapterfusion,
  author = {Pfeiffer, J. et al.},
  title = {AdapterFusion: Non-Destructive Task Composition for Transfer Learning},
  year = {2020},
  journal = {ArXiv}
}

@article{liu2019roberta,
  author = {Liu, Y. et al.},
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  journal = {ArXiv},
  year = {2019}
}

@article{xu2023parameter,
  author = {Xu, L. and Xie, H. and Qin, S.-Z. J. and Tao, X. and Wang, F. L.},
  title = {Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment},
  journal = {ArXiv},
  year = {2023}
}

@article{he2023mera,
  author = {He, S. et al.},
  title = {MerA: Merging Pretrained Adapters For Few-Shot Learning},
  journal = {ArXiv},
  year = {2023}
}

@misc{adapterhub_overview,
  title = {Overview and Configuration — AdapterHub documentation},
  howpublished = {\url{https://docs.adapterhub.ml/overview.html}}
}

@misc{facebookai_roberta_base,
  title = {FacebookAI/roberta-base · Hugging Face},
  howpublished = {\url{https://huggingface.co/FacebookAI/roberta-base}}
}


@misc{allenai_dont_stop_pretraining,
  title = {allenai/dont-stop-pretraining: Code associated with the Don’t Stop Pretraining ACL 2020 paper},
  howpublished = {\url{https://github.com/allenai/dont-stop-pretraining}}
}

@misc{transformers,
  title = {Transformers Library by Hugging Face},
  howpublished = {GitHub repository},
  note = {\url{https://huggingface.co/docs/transformers/en/index}}
}

@misc{adapters,
  title = {AdapterHub: Code associated with adapters},
  howpublished = {Github repository},
  note = {\url{https://github.com/adapter-hub/adapters}}
}
