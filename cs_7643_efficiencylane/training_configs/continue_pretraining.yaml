training_args:
  output_dir: './mlm_model'
  num_train_epochs: 100
  per_device_train_batch_size: 256
  learning_rate: 2e-5
  adam_epsilon: 1e-6
  adam_beta1: 0.9
  adam_beta2: 0.98
  weight_decay: 0.01
  warmup_steps: 39 #6% of 660 (max_steps) = int(np.ceil((n_examples / batch_size) * n_epochs))
  lr_scheduler_type: 'linear'
  save_steps: 1000
  logging_dir: './logs'
  logging_steps: 500
  evaluation_strategy: 'no' #Assume no evaluation if unlabeled data
  report_to: 'none'
