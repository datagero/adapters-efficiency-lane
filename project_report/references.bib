@inproceedings{gururangan2020dont,
  author = {Suchin Gururangan and Ana Marasovic and Swabha Swayamdipta and Kyle Lo and Iz Beltagy and Doug Downey and Noah A. Smith},
  title = {Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks},
  booktitle = {ACL},
  year = {2020}
}

@inproceedings{houlsby2019parameter,
  author = {Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
  title = {Parameter-Efficient Transfer Learning for NLP},
  booktitle = {ICML},
  year = {2019}
}

@article{mccloskey1989catastrophic,
  author = {Michael McCloskey and Neal J. Cohen},
  title = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
  journal = {Psychology of Learning and Motivation},
  year = {1989}
}

@article{pfeiffer2020adapterhub,
  author = {Jonas Pfeiffer and Andreas Rücklé and Clifton Poth and Aishwarya Kamath and Ivan Vulić and Sebastian Ruder and Kyunghyun Cho and Iryna Gurevych},
  title = {AdapterHub: A Framework for Adapting Transformers},
  year = {2020},
  journal = {ArXiv}
}

@article{pfeiffer2020adapterfusion,
  author = {Jonas Pfeiffer and Aishwarya Kamath and Andreas Rücklé and Kyunghyun Cho and Iryna Gurevych},
  title = {AdapterFusion: Non-Destructive Task Composition for Transfer Learning},
  year = {2020},
  journal = {ArXiv}
}

@article{liu2019roberta,
  author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  journal = {ArXiv},
  year = {2019}
}

@article{xu2023parameter,
  author = {Lingling Xu and Haoran Xie and Si-Zhao Joe Qin and Xiaohui Tao and Fu Lee Wang},
  title = {Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment},
  journal = {ArXiv},
  year = {2023}
}

@article{he2023mera,
  author = {Shwai He and Run-Ze Fan and Liang Ding and Li Shen and Tianyi Zhou and Dacheng Tao},
  title = {MerA: Merging Pretrained Adapters For Few-Shot Learning},
  journal = {ArXiv},
  year = {2023}
}

@article{kim2021adapterstapt,
  author = {Seungwon Kim and Alex Shum and Nathan Susanj and Jonathan Hilgart},
  title = {Revisiting Pretraining with Adapters},
  journal = {ACL Anthology},
  year = {2021}
}


@misc{adapterhub_overview,
  title = {AdatperHub: Overview and Configuration},
  howpublished = {\url{https://docs.adapterhub.ml/overview.html}}
}

@misc{facebookai_roberta_base,
  title = {FacebookAI/roberta-base · Hugging Face},
  howpublished = {\url{https://huggingface.co/FacebookAI/roberta-base}}
}


@misc{allenai_dont_stop_pretraining,
  title = {Code associated with the Don’t Stop Pretraining ACL 2020 paper},
  howpublished = {\url{https://github.com/allenai/dont-stop-pretraining}}
}

@misc{transformers,
  title = {Transformers Library by Hugging Face},
  howpublished = {GitHub repository},
  note = {\url{https://huggingface.co/docs/transformers/en/index}}
}

@misc{adapters,
  title = {AdapterHub: Code associated with adapters},
  howpublished = {Github repository},
  note = {\url{https://github.com/adapter-hub/adapters}}
}

@misc{optuna,
  title = {Optuna: An open source hyperparameter optimization framework to automate hyperparameter search},
  howpublished = {\url{https://optuna.org/}}
}


